{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "robinV5i2_materials.ipynb",
   "provenance": [],
   "collapsed_sections": [
    "3lIsidA2WhIy"
   ]
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "source": [
    "# ROBin's Training"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "What this notebook is and not:\n",
    "* This notebook is not the model that ROBin uses.\n",
    "* This notebook is only the orchestrator to training ROBin's material and items model.\n",
    "\n",
    "Due to limitation to resources, ROBin's training has only been limited to google's Colab."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "source": [
    "## Helpers for mounting Gdrive and fetch source from repo"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "import requests\n",
    "from io import BytesIO\n",
    "from zipfile import ZipFile\n",
    "import re\n",
    "import os\n",
    "from shutil import rmtree, copyfile\n",
    "from google.colab import drive"
   ],
   "cell_type": "code",
   "metadata": {
    "id": "EoJM657Kt28g"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "### Mirroring the main repo\n",
    "Since the companies repositories are all private and therefore Colab has no access to it, another public repo is setup on a personal github account to mirror the repository."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_robin_source(git_url):\n",
    "  r = requests.get(git_url)\n",
    "  mem_zip = BytesIO(r.content)\n",
    "\n",
    "  root_dir=None\n",
    "  src_dir=None\n",
    "\n",
    "  with ZipFile(mem_zip, 'r') as zipObj:\n",
    "    pathnames = zipObj.namelist()\n",
    "    idx = list(map(lambda path: bool(re.search(\"src/$\", path)), pathnames)).index(True)\n",
    "    root_dir = pathnames[0]\n",
    "    src_dir = pathnames[idx]\n",
    "    zipObj.extractall()\n",
    "\n",
    "  os.rename(src_dir, 'src')\n",
    "  rmtree(root_dir)"
   ]
  },
  {
   "source": [
    "### Fetching dataset stored on google drive"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "from typing import List, Iterable\n",
    "\n",
    "def fetch_from_gdrive(list_of_zips: List[Iterable]):\n",
    "  drive.mount('drive')\n",
    "\n",
    "  for zip, gdrive_path in list_of_zips:\n",
    "    if not os.path.exists(zip):\n",
    "      print(\"Copying\", zip)\n",
    "      copyfile(src=gdrive_path, dst=zip)\n",
    "    else: \n",
    "      print(zip, \"already exists\")\n",
    "  \n",
    "  drive.flush_and_unmount() # Unmounting Google Drive as a good practice"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "HplaEM4f7VFP"
   },
   "source": [
    "repo_mirror_url = # 'https://github.com/<git_username>/<mirror_repo>/archive/<branch>.zip'\n",
    "download_robin_source(repo_mirror_url)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setting up project structure on Colab"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from src.model_builder import RobinMobilenetV2\n",
    "from src.helpers import create_dir_if_not_exist, split_train_valid_test,\\\n",
    "  get_dataset_distribution\n",
    "from src.pipelines import create_input_pipelines, \\\n",
    "    train_new_model, fine_tune_model, evaluate_model\n",
    "from src.log_utils import plot_class_distribution, log_as_image"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "data_dir=Path(os.getcwd())/'data'\n",
    "\n",
    "images_dir= data_dir/'images'\n",
    "train_dir= data_dir/'train'\n",
    "valid_dir= data_dir/'valid'\n",
    "test_dir= data_dir/'test'\n",
    "\n",
    "model_dir = Path(os.getcwd())/'models'\n",
    "save_model_dir = model_dir/'saved'\n",
    "logs_dir = Path(os.getcwd())/'logs'\n",
    "export_dir = Path(os.getcwd())/'export'\n",
    "\n",
    "project_dir=[\n",
    "  images_dir,\n",
    "  train_dir,\n",
    "  valid_dir,\n",
    "  test_dir,\n",
    "  save_model_dir,\n",
    "  lite_models_dir,\n",
    "  logs_dir,\n",
    "  export_dir\n",
    "]\n",
    "\n",
    "create_dir_if_not_exist(project_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B7S8QFUJsyn3",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Fetch data from drive"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "0QmBdbh1PBgL",
    "outputId": "6f191008-a0c8-46a0-eb9d-1515e491ea86",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "source": [
    "zipped_dataset_name = # Name of zipped images eg. 'materials_ds_v2.zip'\n",
    "gdrive_path_to_dataset = # eg. path to zipped data f\"/content/drive/My Drive/Colab Notebooks/ROBin/datasets/{zipped_dataset_name}\"\n",
    "\n",
    "fetch_from_gdrive([(zipped_dataset_name, gdrive_path_to_dataset)])\n",
    "\n",
    "# Some boilerplate to unzip and remove unnecessary files from project structure\n",
    "zip_path = # update\n",
    "zip_to_dest = # update\n",
    "\n",
    "ZipFile(zip_path).extractall(zip_to_dest)\n",
    "os.remove(zip_path)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "## Split data randomly"
   ],
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "source": [
    "By setting `train_ratio` to 0.7 and `valid_ratio` to 0.15, that leaves a the `test_ratio` to 0.15 as well"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "SiU473eey4br",
    "outputId": "6c327ab6-e381-441f-fe9c-4467430f986e",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "split_train_valid_test(\n",
    "    images_dir,\n",
    "    train_dir,\n",
    "    valid_dir,\n",
    "    test_dir,\n",
    "    train_ratio=0.7,\n",
    "    valid_ratio=0.15\n",
    ")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "## Visualise class distribution \n",
    "\n",
    "Visualising at this point prior to training allows for the scientist to decide if he/she would like to up/down-sample."
   ],
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "B30MMxSLlwGf",
    "outputId": "dee2b3e8-1e59-49dc-c349-b1efbe630520",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    }
   },
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "train_distro = get_dataset_distribution(train_dir)\n",
    "valid_distro = get_dataset_distribution(valid_dir)\n",
    "test_distro = get_dataset_distribution(test_dir)\n",
    "class_names = list(train_distro.keys())\n",
    "\n",
    "distro_plot = plot_class_distribution(\n",
    "    class_names,\n",
    "    [ train_distro[key] for key in train_distro ],\n",
    "    [ valid_distro[key] for key in valid_distro ],\n",
    "    [ test_distro[key] for key in test_distro ],\n",
    ")\n",
    "\n",
    "plt.axis('off')\n",
    "plt.imshow(Image.open(distro_plot))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close() # Closing the figure so that it doesn't affect the plots to be logged for Tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dfFo6Ztxs5hI",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Training"
   ]
  },
  {
   "source": [
    "### Declare the variation the hyperparameters to train with"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "hyperparams_01 = {\n",
    "    \"img_size\": (224, 224),\n",
    "    \"channels\": 3,\n",
    "    \"batch_size\": 16,\n",
    "    \"base_learning_rate\": 0.0001,\n",
    "    \"initial_epochs\": 15,\n",
    "    \"fine_tune_from\": 100,\n",
    "    \"fine_tune_epochs\": 10\n",
    "}\n",
    "\n",
    "hyperparams_02 = {\n",
    "    \"img_size\": (224, 224),\n",
    "    \"channels\": 3,\n",
    "    \"batch_size\": 32,\n",
    "    \"base_learning_rate\": 0.0001,\n",
    "    \"initial_epochs\": 20,\n",
    "    \"fine_tune_from\": 100,\n",
    "    \"fine_tune_epochs\": 15\n",
    "}\n",
    "\n",
    "hyperparams_03 = {\n",
    "    \"img_size\": (224, 224),\n",
    "    \"channels\": 3,\n",
    "    \"batch_size\": 32,\n",
    "    \"base_learning_rate\": 0.0005,\n",
    "    \"initial_epochs\": 25,\n",
    "    \"fine_tune_from\": 100,\n",
    "    \"fine_tune_epochs\": 20\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "source": [
    "### Define the training pipeline\n",
    "\n",
    "In this case, we create a brand new model for each training."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def start_training_new_model(model_builder, hyperparams, training_tag, datasets, logdir, save_to):\n",
    "    # Log Class Distribution\n",
    "    log_as_image(\n",
    "        os.path.join(logdir,'class_distribution'),\n",
    "        \"Class Distribution Plot\",\n",
    "        distro_plot\n",
    "    )\n",
    "\n",
    "    \"\"\"\n",
    "    IMPORTANT!!!\n",
    "    The model instance generated from train_new_model is stateful and its state\n",
    "    is updated across the different pipelines\n",
    "    \"\"\"\n",
    "\n",
    "    # Creates a new model by transfering learning and trains its new classifer layers\n",
    "    (robin_model, history) = train_new_model(\n",
    "    model_builder=model_builder,\n",
    "    hyperparams=hyperparams,\n",
    "    datasets=datasets,\n",
    "    log_dir=logdir,\n",
    "    training_tag=training_tag\n",
    "    )\n",
    "\n",
    "    evaluate_model(\n",
    "        model=robin_model(),\n",
    "        class_names=datasets['class_names'],\n",
    "        test_ds=datasets[\"test_ds\"],\n",
    "        log_dir=logdir,\n",
    "        training_tag=training_tag,\n",
    "        cm_name=\"New Classifier for Materials Model\",\n",
    "        log_false_images=False\n",
    "    )\n",
    "\n",
    "    # Unlocks several layers before the classifer and further trains the model\n",
    "    fine_tune_model(\n",
    "        model_instance=robin_model,\n",
    "        hyperparams=hyperparams,\n",
    "        datasets=datasets,\n",
    "        log_dir=logdir,\n",
    "        training_tag=training_tag,\n",
    "        history=history\n",
    "    )\n",
    "\n",
    "    evaluate_model(\n",
    "        model=robin_model(),\n",
    "        class_names=datasets['class_names'],\n",
    "        test_ds=datasets[\"test_ds\"],\n",
    "        log_dir=logdir,\n",
    "        training_tag=training_tag,\n",
    "        cm_name=\"Fine Tune Materials Model\",\n",
    "        log_false_images=True\n",
    "    )\n",
    "    tf.saved_model.save(robin_model(), save_to)\n",
    "\n",
    "    # Zipping for exporting to external/cloud storage\n",
    "    zip_dir(f'/content/logs/{training_tag}', dst_dir=\"export/logs\")\n",
    "    zip_dir(f'/content/models/{training_tag}', dst_dir=\"export/models\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "source": [
    "### Perform training with the variations of hyerparameters"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "for hyperparams_variation in [hyperparams_01, hyperparams_02, hyperparams_03]:\n",
    "\n",
    "  train_ds, validation_ds, test_ds = create_input_pipelines(\n",
    "      [train_dir, valid_dir, test_dir],\n",
    "      hyperparams_variation\n",
    "  )\n",
    "\n",
    "  dataset = {\n",
    "      \"train_ds\": train_ds,\n",
    "      \"validate_ds\": validation_ds,\n",
    "      \"test_ds\": test_ds,\n",
    "      \"class_names\": train_ds.class_names\n",
    "  }\n",
    "\n",
    "  model_context= #update  \"material\" or \"items\"\n",
    "  training_tag = f\"{datetime.now().strftime('%Y%m%d-%H%M')}_{model_context}\"\n",
    "\n",
    "  save_path = os.path.join(save_model_dir, training_tag)\n",
    "  log_path = os.path.join(logdir, training_tag)\n",
    "\n",
    "  start_training_new_model(\n",
    "      model_builder=RobinMobilenetV2,\n",
    "      hyperparams=hyperparams_variation,\n",
    "      training_tag=training_tag,\n",
    "      datasets=dataset,\n",
    "      logdir=log_path,\n",
    "      save_to = save_path\n",
    "  )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ]
}